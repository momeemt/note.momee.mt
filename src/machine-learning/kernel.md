# カーネル

2クラス分類で線形モデルを使って分離したいが難しい場合、非線形な変換を導入することで線形分離できる。
たとえば多項式特徴量 $\phi(x) = (x^0, x^1, x^2, \dots, x^M)$ を使って次元を上げて線形分離した後に、次元を戻すことで結果的に分離できる。

## 特徴量関数

特徴量関数によって、元の特徴量よりも次元を上げることができる。
次元が高いが線形性を持つことが期待されるので扱いやすくなる。

### 交互作用項を持つ多項式特徴量

- 2次元2次多項式特徴量: $\phi(\boldsymbol{x})^T = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$
- 2次元3次多項式特徴量: $\phi(\boldsymbol{x})^T = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_1^3, x_2^3, x_1^2x_2, x_1x_2^2)$

特徴量同士の積である**交互作用項**が含まれる。

### ガウスカーネル特徴量

$$
\phi_j (x) = \exp{\left\{ -\frac{(x-\mu_j)^2}{2s^2} \right\}}
$$

### シグモイド関数

$$
\phi_j(x) = \sigma \left( \frac{x - \mu_j}{s} \right), \\
\sigma(a) = \frac{1}{1 + \exp{(-a)}}
$$

## 多項式カーネル

特徴量ベクトルの次元数が組み合わせ爆発してしまうので扱えなくなる。
そこで、以下に示される**M次多項式カーネル**を導入する。

$$
k(\boldsymbol{x}, \boldsymbol{x'}) = (\boldsymbol{x}^T \boldsymbol{x'} + 1)^M = \phi(\boldsymbol{x})^T \phi(\boldsymbol{x'})
$$

多項式カーネルを用いることで直接的に特徴量関数を計算せずとも特徴量関数によって得られた2つの値の内積を得ることができる。
よって、内積だけなら非常に高い次元、あるいは無限次元の特徴量であっても有限の効率的な時間で計算できる。
また、正則化すれば過学習を避けることができる。


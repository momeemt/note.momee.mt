# 線形回帰

## 単回帰

- 訓練事例
    - $(x_1, t_1), (x_2, t_2), \dots, (x_N, t_N)$
    - $x_i$
        - **特徴量**
        - 観測可能なデータ
    - $t_i$
        - **目標値**
        - 予測対象となるデータ

学習時には、訓練事例から特徴量と目標値の間の線形モデル $t = wx + b$ を求める。
パラメータ $w$ は特徴量 $x$ と目標値 $t$ の関係の強さで、符号が正なら特徴量が大きい値を取るほど目標値も大きくなり、符号が負なら特徴量が小さい値を取るほど目標値は大きくなる。
パラメータ $b$ は**バイアス**と呼ばれ、特徴量が$0$の時の予測値を表す。

その線形モデルを用いて、目標値が不明の特徴量が与えられたときに、その目標値を予測する。

## 重回帰

- 特徴（独立変数）: $x_i \in \R^D$
- 目標値（従属変数）: $t_i \in \R^1$
- 事例: 特徴と目標値の組 $(x_i, t_i), i = 1, \dots, N$
- 目的
    - 多数の目標値付き事例が与えられた時に、事例から目標値を予測する

重回帰では、複数の特徴量とバイアスから目標値を予測するので、目標値は以下のように計算される。

$$
t = w_0 + w_1x_1 + \dots + w_Dx_D = w_0 + \sum^D_{d=1} w_dx_d
$$

また、事例を次のように定義すると、

$$
\begin{pmatrix}
    \boldsymbol{x}_1^T \\
    \boldsymbol{x}_2^T \\
    \vdots \\
    \boldsymbol{x}_N^T
\end{pmatrix}
=
\begin{pmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1D} \\
    1 & x_{21} & x_{22} & \cdots & x_{2D} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{N1} & x_{N2} & \cdots & x_{ND}
\end{pmatrix}
$$

重回帰モデルを次のように表現できる。

$$
t = \sum^D_{i=0} w_ix_i = \boldsymbol{w}^T \boldsymbol{x}
$$


# 特徴選択

目標変数の予測に必要ない特徴を排除して有用な特徴のみを使ってモデリングする手法。
予測時における計算効率の向上や、目標変数の予測精度の向上、学習結果の解釈性の向上のために行う。

## フィルター法

$D$次元の特徴変数 $x_1, x_2, \dots, x_D$ と目標変数 $t$ があるとき、各特徴変数と目標変数の相関を計算する。
たとえば、ピアソン相関係数 $r_s$ を以下のように求める。

$$
r_s = \frac{\sum^N_{i=1} (\bar{x}_s - x_{si})(\bar{t} - t_i)}{\sqrt{\sum^N_{i=1} (\bar{x}_s - x_{si})^2 \sum^N_{i=1}(\bar{t} - t_i)^2}}
$$

次に、ある閾値 $\theta$ を決めて、それより大きな変数のみを特徴として利用する。
ただし、2つ以上の変数がセットで目標値に影響を与える場合はフィルター法が適用できない。

## ラッパー法

$D$次元の特徴変数と目標変数があるとする。
ここで、特徴のインデックスを $\{1, 2, \dots, D\}$ として、$M$ 個の特徴を選択する。
サイズ$M$のべき集合を列挙して、各要素についてその集合で指定された特徴でモデルを学習させ、そのテスト誤差を評価する。
最後に、もっとも良いテスト誤差を与える特徴部分集合を採用する。

最良の特徴選択が実現でき、複数個の変数がセットで目標値に影響を与える場合にも対応できるが、部分集合の候補数が特徴量に対して指数的に増加する。
そこで、前向き貪欲探索を行うが、最良の特徴選択ができる保証はなく、並び方により最終的な結果が異なる。

## スパース性を導入する正則化の利用

